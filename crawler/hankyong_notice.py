import requests
import pymysql
import hashlib
from bs4 import BeautifulSoup
import urllib3
import os
from dotenv import load_dotenv

# .env λ΅λ“
load_dotenv(dotenv_path="/root/hknu_scraper/.env")

# SSL κ²½κ³  λ¬΄μ‹
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def clean_date(date_str):
    return date_str.strip().replace(".", "-").split("(")[0]

def generate_hash(title, link):
    return hashlib.sha256((title + link).encode("utf-8")).hexdigest()

def run_hankyong_notice():
    print("π“Ά ν•κ²½κµ­λ¦½λ€ν•™κµ ν•κ²½κ³µμ§€")
    print("=" * 30)

    base_url = "https://www.hknu.ac.kr/bbs/kor/69/artclList.do"
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Content-Type": "application/x-www-form-urlencoded"
    }

    conn = pymysql.connect(
        host=os.getenv("DB_HOST"),
        port=int(os.getenv("DB_PORT")),
        user=os.getenv("DB_USER"),
        password=os.getenv("DB_PASSWORD"),
        database=os.getenv("DB_NAME"),
        charset="utf8mb4"
    )
    cursor = conn.cursor()
    cursor.execute("SELECT hash FROM hankyong_notices")
    existing_hashes = set(row[0] for row in cursor.fetchall())

    seen_hashes_in_this_run = set()  # π’΅ ν„μ¬ μ‹¤ν–‰ μ¤‘ μ¤‘λ³µ λ°©μ§€μ© μ„ΈνΈ

    page = 1
    empty_count = 0
    MAX_EMPTY = 3
    new_notices = []

    while True:
        print(f"π“„ νμ΄μ§€ {page} μμ§‘ μ¤‘...")

        data = {
            "layout": "6b6f7240403536314040666e637431",
            "page": str(page),
            "srchColumn": "",
            "srchWrd": "",
            "bbsClSeq": "",
            "bbsOpenWrdSeq": "",
            "rgsBgndeStr": "",
            "rgsEnddeStr": "",
            "isViewMine": "false"
        }

        try:
            res = requests.post(base_url, headers=headers, data=data, verify=False)
        except requests.exceptions.SSLError as e:
            print(f"β SSL μ¤λ¥: {e}")
            break

        if "board-table" not in res.text:
            print("π›‘ κ³µμ§€ ν…μ΄λΈ” μ—†μ λλ” λ§μ§€λ§‰ νμ΄μ§€ λ„λ‹¬")
            break

        soup = BeautifulSoup(res.text, "html.parser")
        rows = soup.select("table.board-table tbody tr")

        if not rows:
            break

        added = 0

        for row in rows:
            title_tag = row.select_one("td.td-subject a")
            date_tag = row.select_one("td.td-date")
            author_tag = row.select_one("td.td-write")

            if not title_tag or not date_tag:
                continue

            title = ' '.join(title_tag.text.split())
            href = "https://www.hknu.ac.kr" + title_tag['href']
            date = clean_date(date_tag.get_text(strip=True))
            author = author_tag.get_text(strip=True) if author_tag else "μ‘μ„±μ μ—†μ"
            hash_val = generate_hash(title, href)

            # DB λλ” ν„μ¬ μ‹¤ν–‰ μ¤‘ μ¤‘λ³µλ κ²½μ° κ±΄λ„λ€
            if hash_val in existing_hashes or hash_val in seen_hashes_in_this_run:
                continue

            seen_hashes_in_this_run.add(hash_val)  # ν„μ¬ μ‹¤ν–‰ μ¤‘ ν•΄μ‹ μ €μ¥
            print(f"π“ {title} | {date} | {author}")
            new_notices.append((title, date, author, href, hash_val))
            added += 1

        if added == 0:
            empty_count += 1
        else:
            empty_count = 0

        if empty_count >= MAX_EMPTY:
            print(f"π›‘ {MAX_EMPTY}νμ΄μ§€ μ—°μ†μΌλ΅ μƒλ΅μ΄ κ³µμ§€κ°€ μ—†μ–΄ μΆ…λ£ν•©λ‹λ‹¤.")
            break

        page += 1

    if new_notices:
        sql = """
        INSERT INTO hankyong_notices (title, notice_date, author, link, hash)
        VALUES (%s, %s, %s, %s, %s)
        """
        cursor.executemany(sql, new_notices)
        conn.commit()
        print(f"β… {len(new_notices)}κ±΄μ μƒ κ³µμ§€λ¥Ό μ €μ¥ν–μµλ‹λ‹¤.")
    else:
        print("β… μƒλ΅μ΄ κ³µμ§€κ°€ μ—†μµλ‹λ‹¤.")

    cursor.close()
    conn.close()